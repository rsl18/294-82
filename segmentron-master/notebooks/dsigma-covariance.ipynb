{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Varieties of Covariance\n",
    "\n",
    "The parameterization of spherical, diagonal, and full covariances offers a progression of richer receptive fields.\n",
    "\n",
    "These forms of the covariance $\\Sigma$ are\n",
    "\n",
    "- **spherical** $\\Sigma = \\begin{bmatrix}\\sigma & 0 \\\\ 0 & \\sigma\\end{bmatrix}$,\n",
    "- **diagonal** $\\Sigma = \\begin{bmatrix}\\sigma_y & 0 \\\\ 0 & \\sigma_x\\end{bmatrix}$, and\n",
    "- **full** $\\Sigma = \\begin{bmatrix}\\sigma_y & \\rho \\\\ \\rho & \\sigma_x\\end{bmatrix}$,\n",
    "\n",
    "with $\\rho$ the correlation of $x, y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap\n",
    "\n",
    "# work from project root for local imports\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# root here refers to the segmentron-master folder\n",
    "root_dir = Path(subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).strip().decode(\"utf-8\"))\n",
    "root_dir = root_dir / \"segmentron-master\"\n",
    "os.chdir(root_dir)\n",
    "sys.path.append(str(root_dir))\n",
    "\n",
    "from sigma.blur import blur2d_full, gauss2d_full, sigma2logchol, logchol2sigma\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before inspecting covariance, let's glance at variance in 1D, by forming Gaussians through the unnormalized density $\\exp{\\frac{-x^2}{2\\sigma^2}}$.\n",
    "We approximate the continuous distribution with the *sampled* Gaussian at linearly-interpolated points, truncate at two standard deviations to include 95\\% of the density, then normalize.\n",
    "Note there is a *discrete* Gaussian approach, derived through diffusion with discrete time, but that's a step for another day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([1.0, 2., 4.])\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, s in enumerate(sigma, 1):\n",
    "    # determine kernel size to cover +/- 2 sigma s.t. 95% of density is included\n",
    "    half_size = int(max(1, torch.ceil(s * 2.)))\n",
    "    # always make odd kernel to keep coordinates centered\n",
    "    kernel_size = half_size*2 + 1\n",
    "    # calculate unnormalized density then normalize\n",
    "    x = torch.linspace(-half_size, half_size, steps=kernel_size)\n",
    "    filter_ = torch.exp(-x**2 / (2*s**2))\n",
    "    filter_sum = filter_.sum()\n",
    "    filter_norm = filter_ / filter_sum\n",
    "\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.title(f\"Gaussian $\\sigma=${s:0.1f}\")\n",
    "    plt.plot(x.numpy(), filter_norm.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider spherical covariance in 2D, which analogously adjusts the scale of the filter.\n",
    "This case reduces to the product of a 1D Gaussian, because spherical covariance gives an *isotropic* distribution that is identical in every direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([1.0, 2., 4.])\n",
    "\n",
    "half_size = int(max(1, torch.ceil(sigma.max() * 2.))) * 1.1\n",
    "x = torch.linspace(-half_size, half_size, steps=kernel_size * 3)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, s in enumerate(sigma, 1):\n",
    "    # calculate unnormalized density then normalize\n",
    "    filter_ = torch.exp(-x**2 / (2*s**2))\n",
    "    # 2D is product of 1D b.c. this is isotropic\n",
    "    filter_ = filter_.view(-1, 1) @ filter_.view(1, -1)\n",
    "    filter_sum = filter_.sum()\n",
    "    filter_norm = filter_ / filter_sum\n",
    "\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.title(f\"Gaussian $\\sigma=${s:0.1f}\")\n",
    "    plt.contour(filter_norm.numpy(), cmap='viridis')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are useful special cases of the Gaussian for composition:\n",
    "\n",
    "- $\\sigma \\to 0$ gives the delta filter, yielding the identity;\n",
    "- $\\sigma < 1$ gives a small, identity-like filter, which can learn to be sharper or smoother, for a good initialization;\n",
    "- $\\sigma \\to \\infty$ gives average pooling;\n",
    "\n",
    "so that smoothing is learnable, can reduce to the identity if no smoothing is desired, and can reduce to averaging (approximately) if global pooling is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([0.1, 0.7, 16.])\n",
    "\n",
    "half_size = 7  # hardcode to focus on center of filter\n",
    "kernel_size = half_size * 2 + 1\n",
    "x = torch.linspace(-half_size, half_size, steps=kernel_size)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, s in enumerate(sigma, 1):\n",
    "    # calculate unnormalized density then normalize\n",
    "    filter_ = torch.exp(-x**2 / (2*s**2))\n",
    "    # 2D is product of 1D b.c. this is isotropic\n",
    "    filter_ = filter_.view(-1, 1) @ filter_.view(1, -1)\n",
    "    filter_sum = filter_.sum()\n",
    "    filter_norm = filter_ / filter_sum\n",
    "\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.title(f\"Gaussian $\\sigma=${s:0.1f}\")\n",
    "    plt.imshow(filter_norm[5:10, 5:10].numpy(), vmin=0, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general covariance, we need the general multivariate Gaussian density (again, unnormalized): $\\text{det}(\\Sigma)^{1/2} \\exp\\left\\{-\\frac{1}{2} x' \\Sigma^{-1} x\\right\\}$.\n",
    "Let's inspect densities with spherical, diagonal, and full covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_sphere = torch.diag(torch.Tensor([2., 2.]))\n",
    "sigma_diag = torch.diag(torch.Tensor([2., 4.]))\n",
    "sigma_full = torch.Tensor([[1., 1.5],\n",
    "                           [1.5, 4.]])\n",
    "sigmas = list(zip(('sphere', 'diag.', 'full'), (sigma_sphere, sigma_diag, sigma_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see sigma.blur.gauss2d_full\n",
    "def sigma_filter(sigma):\n",
    "    half_size = torch.ceil(sigma.diag()**(0.5) * 2.).clamp(min=1.).int()\n",
    "    kernel_size = half_size*2 + 1\n",
    "    y = torch.linspace(half_size[0], -half_size[0], steps=kernel_size[0])\n",
    "    x = torch.linspace(-half_size[1], half_size[1], steps=kernel_size[1])\n",
    "    coords = torch.stack(torch.meshgrid(y, x), dim=-1).view(-1, 2)\n",
    "    # vectorize quadratic form x^T sigma x by matmul-product-sum\n",
    "    filter_ = torch.det(sigma)**(-0.5) * torch.exp(-(0.5)*(((coords @ sigma.inverse()) * coords).sum(1)))\n",
    "    filter_ /= filter_.sum()\n",
    "    filter_ = filter_.view(*kernel_size)\n",
    "    return filter_\n",
    "    \n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "filters = []\n",
    "for name, s in sigmas:\n",
    "    filters.append((name, sigma_filter(s)))\n",
    "    \n",
    "max_size, _ = torch.stack([torch.tensor(f.size()) for name, f in filters]).max().repeat(2)\n",
    "for i, (name, f) in enumerate(filters, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.title(name)\n",
    "    # pad to equal size for display\n",
    "    pad_h, pad_w = (max_size - torch.tensor(f.size()))  // 2\n",
    "    f = torch.nn.functional.pad(f, (pad_w, pad_w, pad_h, pad_h))\n",
    "    plt.imshow(f.numpy())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calculating the density at linearly-interpolated coordinates, let's inspect the placement of coordinates by mapping the coordinates of a standard Gaussian through the covariance.\n",
    "First, let's look out the coordinates for a 1D Gaussian when linearly interpolating through the inverse cumulative distribution function, for a sense of the spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([1., 2., 4.])\n",
    "eps = torch.tensor(0.025)  # two sigma coverage for 95% of density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 5\n",
    "\n",
    "fig = plt.figure(figsize=(15, 1))\n",
    "for i, s in enumerate(sigma, 1):\n",
    "    normal = torch.distributions.normal.Normal(torch.tensor(0.), s)\n",
    "    x = normal.icdf(torch.linspace(eps, -eps + 1, steps=num_steps))\n",
    "    subplt = plt.subplot(1, 3, i)\n",
    "    plt.title(f\"$\\sigma$ = {s:}\")\n",
    "    plt.plot(x.numpy(), torch.zeros_like(x).numpy(), linestyle='none', marker='.')\n",
    "    plt.ylim(-10., 10)\n",
    "    plt.xlim(-16., 16)\n",
    "    subplt.get_yaxis().set_visible(False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2D, we switch to polar coordinates and choose the radii according to the spacing above.\n",
    "We choose the number of angles for angular resolution, and the number of steps for distance resolution (where steps count the rings from the center).\n",
    "We take a two sigma truncation and plot the coordinates for different standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "num_angles = 8\n",
    "num_steps = 2\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, s in enumerate(sigma, 1):\n",
    "    normal = torch.distributions.normal.Normal(torch.tensor(0.), s)\n",
    "    angles = torch.linspace(0., 2*math.pi, steps=num_angles + 1)[:-1]  # exclude end == beginning\n",
    "    radii = normal.icdf(torch.linspace(0.5, -eps + 1, steps=num_steps + 1))[1:]  # steps\n",
    "\n",
    "    rho, theta = torch.meshgrid((radii, angles))\n",
    "    y = rho * torch.sin(theta)\n",
    "    x = rho * torch.cos(theta)\n",
    "    y = torch.cat((torch.tensor([0.]), y.view(-1)))\n",
    "    x = torch.cat((torch.tensor([0.]), x.view(-1)))\n",
    "\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.title(f\"$\\sigma$ = {s:}\")\n",
    "    plt.plot(x.numpy(), y.numpy(), linestyle='none', marker='.')\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the coordinates of the standard Gaussian through the covariance gives us a sampled approximation to that Gaussian.\n",
    "Adjusting the number of steps and angles controls the quality for trading accuracy and computation.\n",
    "Coloring the points shows the correspondences among the covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1\n",
    "num_angles = 8\n",
    "\n",
    "# make a standard Gaussian\n",
    "normal = torch.distributions.normal.Normal(torch.tensor(0.), 1.)\n",
    "angles = torch.linspace(0., 2*math.pi, steps=num_angles + 1)[:-1]\n",
    "radii = normal.icdf(torch.linspace(0.5, -eps + 1, steps=num_steps + 1))[1:]\n",
    "\n",
    "rho, theta = torch.meshgrid((radii, angles))\n",
    "y = rho * torch.sin(theta)\n",
    "x = rho * torch.cos(theta)\n",
    "coords = torch.cat((torch.zeros(1, 2),\n",
    "                    torch.stack((y, x), dim=-1).view(-1, 2)))\n",
    "\n",
    "# define a variety of full covariances\n",
    "sigmas = [\n",
    "        torch.tensor([1., 0., 0., 1.]).view(-1, 2),\n",
    "        torch.tensor([0.1, 0., 0., 0.1]).view(-1, 2),\n",
    "        torch.tensor([2., 0., 0., 2.]).view(-1, 2),\n",
    "        torch.tensor([2., 0., 0., 0.5]).view(-1, 2),\n",
    "        torch.tensor([0.5, 0., 0., 2.0]).view(-1, 2),\n",
    "        torch.tensor([1., 0.5, 0.5, 1.]).view(-1, 2),\n",
    "        torch.tensor([1., -0.5, -0.5, 1.]).view(-1, 2),\n",
    "]\n",
    "\n",
    "# transform standard coordinates by covariances\n",
    "lim = 5.\n",
    "plt.figure(figsize=(3*len(sigmas), 3))\n",
    "for i, s in enumerate(sigmas, 1):\n",
    "    s_coords = coords.clone() @ s\n",
    "    y, x = torch.unbind(s_coords, dim=1)\n",
    "\n",
    "    plt.subplot(1, len(sigmas), i)\n",
    "    for j in range(len(s_coords)):\n",
    "        plt.plot(x[j].numpy(), y[j].numpy(), linestyle='none', marker='.', markersize=12)\n",
    "    plt.xlim(-lim, lim)\n",
    "    plt.ylim(-lim, lim)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compose our approximate Gaussian coordinates with a filter, we must map from the Gaussian points to the filter taps.\n",
    "Here we make a mask group the center point and tap together and group the other points and taps by angle.\n",
    "We normalize the mask such that each tap has equal weighting and the total weighting of the points is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points x taps for 3x3 kernel\n",
    "num_taps = 3 * 3\n",
    "mask = torch.zeros(num_steps * num_angles + 1, num_taps).float()\n",
    "# assign center point to center tap\n",
    "mask[0, num_taps // 2] = 1.\n",
    "# assign angles to their taps: \n",
    "# - angles go in counter-clockwise order from center-right\n",
    "# - taps go in row major order from top-left\n",
    "mask[1::num_angles, 5] = 1.\n",
    "mask[2::num_angles, 2] = 1.\n",
    "mask[3::num_angles, 1] = 1.\n",
    "mask[4::num_angles, 0] = 1.\n",
    "mask[5::num_angles, 3] = 1.\n",
    "mask[6::num_angles, 6] = 1.\n",
    "mask[7::num_angles, 7] = 1.\n",
    "mask[8::num_angles, 8] = 1.\n",
    "# normalize so every point-tap dot has equal weight,\n",
    "# and all the points together sum to one, so that \n",
    "# the input magnitude is unchanged\n",
    "mask /= mask.sum(0).view(1, -1)\n",
    "mask /= mask.sum()\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizing $\\Sigma$ for Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance $\\Sigma$ is not any given $d \\times d$ matrix, and in particular it's positive definite, so we have to properly parametrize it for unconstrained optimization. For a primer on covariance parameterization, see \n",
    "\n",
    "> Unconstrained Parameterizations for Variance-Covariance Matrices. Pinheiro & Bates 1996.\n",
    " \n",
    "In our case, the log-Cholesky parameterization is a good choice because it's simple and quick:\n",
    "$\\Sigma = U'U$ for upper-triangular $U$ with positive diagonal.\n",
    "We can keep the diagonal positive by representing the log of the diagonal (hence log-Cholesky), and exp'ing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random covariances in log-Cholesky form\n",
    "params = torch.randn(3)\n",
    "\n",
    "U = torch.zeros((2, 2))\n",
    "triu_indices = [[0, 0, 1], [0, 1, 1]]\n",
    "U[triu_indices] = params\n",
    "U.diagonal().exp_()\n",
    "U_inv = U.inverse()\n",
    "\n",
    "sigma = U.t() @ U\n",
    "sigma_inv = U_inv @ U_inv.t()\n",
    "\n",
    "print(f\"parameters: {params}\")\n",
    "print(f\"sigma:\\n{sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert known sigma to log-Cholesky parameterization to check that the original and log-Cholesky kernels agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_filter_logchol(params):\n",
    "    # make cholesky factor and inverse\n",
    "    U = torch.zeros((2, 2))\n",
    "    triu_indices = [[0, 0, 1], [0, 1, 1]]\n",
    "    U[triu_indices] = params\n",
    "    U.diagonal().exp_()\n",
    "    U_inv = U.inverse()\n",
    "    # make filter\n",
    "    half_size = torch.ceil((U.t() @ U).diag()**(0.5) * 2.).clamp(min=1.).int()\n",
    "    kernel_size = half_size*2 + 1\n",
    "    y = torch.linspace(half_size[0], -half_size[0], steps=kernel_size[0])\n",
    "    x = torch.linspace(-half_size[1], half_size[1], steps=kernel_size[1])\n",
    "    coords = torch.stack(torch.meshgrid(y, x), dim=-1).view(-1, 2)\n",
    "    filter_ = torch.det(U)**-1. * torch.exp(-(0.5)*(((coords @ (U_inv @ U_inv.t())) * coords).sum(1)))\n",
    "    filter_ /= filter_.sum()\n",
    "    filter_ = filter_.view(*kernel_size)\n",
    "    return filter_\n",
    "\n",
    "# make filter through sigma\n",
    "filter_ = sigma_filter(sigma_full)\n",
    "\n",
    "# take cholesky decomposition of sigma, extract log-Cholesky params\n",
    "full_chol = torch.cholesky(sigma_full, upper=True).contiguous().view(-1)\n",
    "full_params = torch.stack((torch.log(full_chol[0]), full_chol[1], torch.log(full_chol[-1])))\n",
    "# make filter through log cholesky params\n",
    "chol_filter_ = sigma_filter_logchol(full_params)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('original filter')\n",
    "plt.imshow(filter_)\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('log-Cholesky filter')\n",
    "plt.imshow(chol_filter_)\n",
    "plt.axis('off')\n",
    "\n",
    "print(\"filter MSE: \", torch.mean((filter_ - chol_filter_)**2).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Experiment: Optimize $\\Sigma$ to Recover Full Covariance Blur in 2D\n",
    "\n",
    "To illustrate receptive field optimization via sigma with a toy problem, let's recover the size of a Gaussian blur from smoothed data in 2D.\n",
    "\n",
    "1. Generate a random 2D signal and smooth it with a reference sigma.\n",
    "2. Instantiate our filter with zero initialization of the sigma parameter.\n",
    "3. Learn sigma by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 64, 64)\n",
    "true_sigma = sigma_full\n",
    "xf = blur2d_full(x, sigma2logchol(true_sigma), std_devs=2).detach()\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('signal')\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('smoothed')\n",
    "plt.imshow(xf.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recovery(xf, xf_hat, g, iter_):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.title(\"Recovery iter. {}\".format(iter_))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(xf.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(xf_hat.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(g.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "cov = torch.nn.Parameter(torch.zeros(3))\n",
    "opt = torch.optim.Adamax([cov], lr=0.1)\n",
    "\n",
    "max_iter = 100\n",
    "for iter_ in range(max_iter):\n",
    "    xf_hat = blur2d_full(x, cov, std_devs=2)\n",
    "    diff = xf_hat - xf\n",
    "    loss = (diff**2).mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    g = gauss2d_full(cov, std_devs=2)\n",
    "    \n",
    "    if iter_ % 10 == 0:\n",
    "        print(f\"iter {iter_:04d} loss {loss.item()}\")\n",
    "    if iter_ in (0, 4, 16):\n",
    "        plot_recovery(xf, xf_hat, g, iter_)\n",
    "print(f\"iter {iter_:04d} loss {loss.item()}\")\n",
    "plot_recovery(xf, xf_hat, g, iter_ + 1)\n",
    "\n",
    "\n",
    "print(\"\\ntrue sigma\\n{}\\nrecovered sigma\\n{}\".format(true_sigma.detach().numpy(), logchol2sigma(cov).detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that optimizing over full covariance can recover a simpler, spherical covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sigma = sigma_sphere\n",
    "xf = blur2d_full(x, sigma2logchol(true_sigma)).detach()\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('signal')\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('smoothed')\n",
    "plt.imshow(xf.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recovery(xf, xf_hat, g, iter_):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.title(\"Recovery iter. {}\".format(iter_))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(xf.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(xf_hat.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(g.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "cov = torch.nn.Parameter(torch.zeros(3))\n",
    "opt = torch.optim.Adamax([cov], lr=0.1)\n",
    "\n",
    "max_iter = 100\n",
    "for iter_ in range(max_iter):\n",
    "    xf_hat = blur2d_full(x, cov)\n",
    "    diff = xf_hat - xf\n",
    "    loss = (diff**2).mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    g = gauss2d_full(cov)\n",
    "    \n",
    "    if iter_ % 10 == 0:\n",
    "        print(f\"iter {iter_:04d} loss {loss.item()}\")\n",
    "    if iter_ in (0, 4, 16):\n",
    "        plot_recovery(xf, xf_hat, g, iter_)\n",
    "print(f\"iter {iter_:04d} loss {loss.item()}\")\n",
    "plot_recovery(xf, xf_hat, g, iter_ + 1)\n",
    "\n",
    "\n",
    "print(\"\\ntrue sigma\\n{}\\nrecovered sigma\\n{}\".format(true_sigma.detach().numpy(), logchol2sigma(cov).detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a little more visual interest and historical reference, let's recover a full covariance blur on a real image: a portrait of Carl Friedrich Gauss himself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image of Gauss, convert to standard 4d float array, and normalize\n",
    "im_gauss = torch.tensor(np.array(Image.open(root_dir / 'notebooks/gauss.jpg'))).float()\n",
    "im_gauss = im_gauss.view(1, 1, *im_gauss.size())\n",
    "im_gauss = (im_gauss - im_gauss.min()) / (im_gauss.max() - im_gauss.min())\n",
    "# shrink and pad for convenience\n",
    "im_gauss = F.interpolate(im_gauss, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "#im_gauss = F.pad(im_gauss, (50, 50, 50, 50), value=1.)\n",
    "\n",
    "# blur Gauss with known Gaussian\n",
    "true_sigma = sigma_full * 4\n",
    "g = gauss2d_full(sigma2logchol(true_sigma))\n",
    "blur_gauss = blur2d_full(im_gauss, sigma2logchol(true_sigma))\n",
    "\n",
    "# crop to remove border effects\n",
    "im_gauss = F.pad(im_gauss, (-20, -20, -20, -20), value=1.)\n",
    "blur_gauss = F.pad(blur_gauss, (-20, -20, -20, -20), value=1.)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(im_gauss.squeeze().numpy())\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(blur_gauss.squeeze().numpy())\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(g.squeeze().numpy())\n",
    "plt.axis('off')\n",
    "\n",
    "x, xf = im_gauss, blur_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recovery(xf, xf_hat, g, iter_):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.title(\"Recovery iter. {}\".format(iter_))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(xf.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(xf_hat.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(g.squeeze().detach().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "cov = torch.nn.Parameter(torch.zeros(3))\n",
    "opt = torch.optim.Adamax([cov], lr=0.1)\n",
    "\n",
    "max_iter = 256\n",
    "for iter_ in range(max_iter):\n",
    "    xf_hat = blur2d_full(x, cov)\n",
    "    #diff = xf_hat - xf\n",
    "    diff = xf_hat[..., 20:-20, 20:-20] - xf[..., 20:-20, 20:-20]\n",
    "    loss = (diff**2).mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    g = gauss2d_full(cov)\n",
    "    \n",
    "    if iter_ % 10 == 0:\n",
    "        print(f\"iter {iter_:04d} loss {loss.item()}\")\n",
    "    if iter_ in (0, 4, 16):\n",
    "        plot_recovery(xf, xf_hat, g, iter_)\n",
    "print(f\"iter {iter_:04d} loss {loss.item()}\")\n",
    "plot_recovery(xf, xf_hat, g, iter_ + 1)\n",
    "\n",
    "\n",
    "print(\"\\ntrue sigma\\n{}\\nrecovered sigma\\n{}\".format(true_sigma.detach().numpy(), logchol2sigma(cov).detach().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
