{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Filter Scale by $\\partial\\sigma$ (with a PyTorch Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# work from the project root\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# root here refers to the segmentron-master folder\n",
    "root_dir = Path(subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).strip().decode(\"utf-8\"))\n",
    "root_dir = root_dir / \"segmentron-master\"\n",
    "os.chdir(root_dir)\n",
    "sys.path.append(str(root_dir))\n",
    "\n",
    "from sigma.blur import blur1d, blur2d_sphere\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Experiment: Optimize $\\sigma$ to Recover Blur Kernel in 1D\n",
    "\n",
    "To illustrate the optimization of kernel size via sigma with a toy problem, let's recover the size of a Gaussian blur from smoothed data in 1D.\n",
    "\n",
    "1. Generate a random 1D signal and smooth it with a reference sigma.\n",
    "2. Instantiate our filter with zero initialization of the sigma parameter.\n",
    "3. Learn sigma by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 100)\n",
    "true_sigma = torch.tensor(3.)\n",
    "xf = blur1d(x, true_sigma, std_devs=2).detach()\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('signal')\n",
    "plt.plot(x.squeeze().numpy())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('smoothed')\n",
    "plt.plot(xf.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recovery(xf, xf_hat, iter_):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.title(\"Recovery iter. {}\".format(iter_))\n",
    "    plt.plot(xf.squeeze().detach().numpy(), 'b', label='ref.')\n",
    "    plt.plot(xf_hat.squeeze().detach().numpy(), 'r', label='rec.')\n",
    "    plt.legend()\n",
    "    \n",
    "scale = torch.nn.Parameter(torch.tensor(0.))\n",
    "opt = torch.optim.SGD([scale], lr=1.0)\n",
    "\n",
    "max_iter = 100\n",
    "for iter_ in range(max_iter):\n",
    "    xf_hat = blur1d(x, scale.exp(), std_devs=2)\n",
    "    diff = xf_hat - xf\n",
    "    loss = 0.5 * (diff**2).mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    if iter_ % 10 == 0:\n",
    "        print('loss ', loss.item())\n",
    "    if iter_ in (0, 4, 16):\n",
    "        plot_recovery(xf, xf_hat, iter_)\n",
    "plot_recovery(xf, xf_hat, iter_ + 1)\n",
    "\n",
    "print('\\ntrue sigma {:0.2f} recovered sigma {:0.2f}'.format(true_sigma.item(), scale.exp().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the gradient by finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.tensor(1e-4)\n",
    "# check gradient at random scales\n",
    "for _ in range(10):\n",
    "    scale = torch.nn.Parameter((torch.randn(1))[0])\n",
    "    # forward-backward\n",
    "    xf_hat = blur1d(x, scale.exp())\n",
    "    loss = 0.5 * ((xf_hat - xf)**2).mean()\n",
    "    loss.backward()\n",
    "    grad = scale.grad\n",
    "    \n",
    "    # forward +eps\n",
    "    xf_eps = blur1d(x, (scale + eps).exp(), std_devs=2)\n",
    "    loss_eps = 0.5 * ((xf_eps - xf)**2).mean()\n",
    "    grad_eps = (loss_eps - loss) / eps\n",
    "    err = torch.abs(grad - grad_eps)\n",
    "    print('analytic {: 09.5f} numerical {: 09.5f} error {:0.8f}'.format(grad.item(), grad_eps.item(), err.item()))\n",
    "    assert(err < 10*eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Experiment: Optimize $\\sigma$ to Recover Blur Kernel in 2D\n",
    "\n",
    "To illustrate the optimization of kernel size via sigma with a toy problem, let's recover the size of a Gaussian blur from smoothed data in 2D.\n",
    "\n",
    "1. Generate a random 2D signal and smooth it with a reference sigma.\n",
    "2. Instantiate our filter with zero initialization of the sigma parameter.\n",
    "3. Learn sigma by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 32, 32)\n",
    "true_sigma = torch.tensor(3.)\n",
    "xf = blur2d_sphere(x, true_sigma).detach()\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('signal')\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('smoothed')\n",
    "plt.imshow(xf.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recovery(xf, xf_hat, iter_):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.title(\"Recovery iter. {}\".format(iter_))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(xf.squeeze().detach().numpy())\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(xf_hat.squeeze().detach().numpy())\n",
    "    \n",
    "scale = torch.nn.Parameter(torch.tensor(0.))\n",
    "opt = torch.optim.SGD([scale], lr=1.0)\n",
    "\n",
    "max_iter = 500\n",
    "for iter_ in range(max_iter):\n",
    "    xf_hat = blur2d_sphere(x, scale.exp())\n",
    "    diff = xf_hat - xf\n",
    "    loss = 0.5 * (diff**2).mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    if iter_ % 50 == 0:\n",
    "        print('loss ', loss.item())\n",
    "    if iter_ in (0, max_iter // 16, max_iter // 4):\n",
    "        plot_recovery(xf, xf_hat, iter_)\n",
    "plot_recovery(xf, xf_hat, iter_ + 1)\n",
    "\n",
    "print('\\ntrue sigma {:0.2f} recovered sigma {:0.2f}'.format(true_sigma.item(), scale.exp().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the gradient by finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.tensor(1e-4)\n",
    "# check gradient at random scales\n",
    "for _ in range(10):\n",
    "    scale = torch.nn.Parameter((torch.randn(1))[0])\n",
    "    # forward-backward\n",
    "    xf_hat = blur2d_sphere(x, scale.exp())\n",
    "    loss = 0.5 * ((xf_hat - xf)**2).mean()\n",
    "    loss.backward()\n",
    "    grad = scale.grad\n",
    "    \n",
    "    # forward +eps\n",
    "    xf_eps = blur2d_sphere(x, (scale + eps).exp())\n",
    "    loss_eps = 0.5 * ((xf_eps - xf)**2).mean()\n",
    "    grad_eps = (loss_eps - loss) / eps\n",
    "    err = torch.abs(grad - grad_eps)\n",
    "    print('analytic {: 09.5f} numerical {: 09.5f} error {:0.8f}'.format(grad.item(), grad_eps.item(), err.item()))\n",
    "    assert(err < 10*eps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
