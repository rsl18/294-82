{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Filter Scale by $\\partial\\sigma$ (in PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Filter and Gradient w.r.t. $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a Gaussian filter from the Gaussian distribution with mean zero and a given standard deviation $\\sigma$. \n",
    "The Gaussian distribution is discretized and truncated to give a filter with finite support for efficient filtering by convolution. \n",
    "For simplicity, we calculate the unnormalized Gaussian density then normalize it by dividing by the sum, which also corrects for the missing density discarded by truncation.\n",
    "The quality of the filter w.r.t. to the true Gaussian distribution is determined by the number of elements in the filter, or kernel size.\n",
    "By parameterizing the kernel size in terms of the standard deviation, we can control the effect of truncation, and for instance include 95% of the true density by making the support cover two standard deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.Tensor([3.])\n",
    "\n",
    "# determine kernel size to cover +/- 2 sigma s.t. >95% of density is included\n",
    "half_size = int(max(1, torch.ceil(sigma*3)))\n",
    "# always make odd kernel to keep coordinates centered\n",
    "kernel_size = half_size*2 + 1\n",
    "# calculate unnormalized density then normalize\n",
    "x = torch.linspace(-half_size, half_size, steps=kernel_size)\n",
    "filter_ = torch.exp(-x**2 / (2*sigma**2))\n",
    "filter_sum = filter_.sum()\n",
    "filter_norm = filter_ / filter_sum\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Gaussian filter\")\n",
    "plt.plot(x.numpy(), filter_norm.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiate the normalized filter w.r.t. standard deviation $\\sigma$ through the chain and quotient rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_filter_sigma = filter_ * x**2 / sigma**3\n",
    "d_filter_sigma_sum = d_filter_sigma.sum()\n",
    "d_filter_norm_sigma = (filter_sum * d_filter_sigma - filter_ * d_filter_sigma_sum) / filter_sum**2\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Gradient of filter w.r.t. $\\sigma$\")\n",
    "plt.plot(x.numpy(), d_filter_norm_sigma.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package up filter and gradient computation so that backward can re-use forward computation of the filter and sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter_with_grad(sigma):\n",
    "    # determine kernel size to cover +/- 2 sigma s.t. >95% of density is included\n",
    "    # always make odd kernel to keep coordinates centered\n",
    "    half_size = int(max(1, torch.ceil(sigma*3)))\n",
    "    kernel_size = half_size*2 + 1\n",
    "    # calculate unnormalized density then normalize\n",
    "    x = torch.linspace(-half_size, half_size, steps=kernel_size)\n",
    "    filter_ = torch.exp(-x**2 / (2*sigma**2))\n",
    "    filter_sum = filter_.sum()\n",
    "    filter_norm = filter_ / filter_sum\n",
    "    # gradient w.r.t. sigma\n",
    "    d_filter_sigma = (filter_ * x**2) / sigma**3\n",
    "    d_filter_sigma_sum = d_filter_sigma.sum()\n",
    "    d_filter_norm_sigma = (filter_sum * d_filter_sigma - filter_ * d_filter_sigma_sum) / filter_sum**2\n",
    "    return filter_norm, d_filter_norm_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizing $\\sigma$ for Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametrize sigma through a learned parameter $s$ and define the gradient of the parameter. For unconstrained optimization, we define the parameter in [-inf, +inf] and map it into a valid sigma with a minimum and default of our choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum sigma = 0.3 gives a little bit of blur, but is closer than not to a delta\n",
    "MIN_SIGMA = torch.tensor([0.3])\n",
    "# sigma parameter shift determines the blur at zero (to make default delta-like)\n",
    "# remember, this also determines the basin for weight decay\n",
    "SIGMA_PARAM_SHIFT = torch.tensor([-3.])\n",
    "\n",
    "# the sigma parameter, s\n",
    "s = torch.tensor([0.])\n",
    "\n",
    "def sigma_from_param(s):\n",
    "    # map to valid sigma by the soft max (not the softmax!)\n",
    "    # https://www.johndcook.com/blog/2010/01/13/soft-maximum/\n",
    "    sigma = torch.log(torch.exp(MIN_SIGMA) + torch.exp(s + SIGMA_PARAM_SHIFT))\n",
    "    return sigma\n",
    "\n",
    "def sigma_param_grad(s):\n",
    "    # gradient by chain rule\n",
    "    d_param = torch.exp(s + SIGMA_PARAM_SHIFT) / (torch.exp(MIN_SIGMA) + torch.exp(s + SIGMA_PARAM_SHIFT))\n",
    "    return d_param\n",
    "\n",
    "\n",
    "sigma = sigma_from_param(s)\n",
    "filter_, _ = gaussian_filter_with_grad(sigma)\n",
    "plt.figure()\n",
    "plt.title(\"Gaussian filter for s = 0\")\n",
    "plt.plot(filter_.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter with differentiable smoothing: forward filters with a given sigma parameter and backward differentiates w.r.t. the sigma parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_filter_forward(x, s):\n",
    "    f, d_f_sigma = gaussian_filter_with_grad(sigma_from_param(s))\n",
    "    xf = F.conv1d(x.view(1, 1, -1), f.view(1, 1, -1), padding=f.size(0) // 2)  # \"same\" convolution\n",
    "    xf = xf[0, 0]  # discard dummy dims\n",
    "    return xf, d_f_sigma\n",
    "\n",
    "def sigma_filter_backward(x, s, d_loss, d_f_sigma):\n",
    "    half_width = d_f_sigma.size(0) // 2\n",
    "    d_f_pad = F.pad(d_loss, (half_width, half_width))\n",
    "    # n.b. we don't flip the data x b.c. convolution in pytorch is cross-correlation\n",
    "    d_f = F.conv1d(d_f_pad.view(1, 1, -1), x.view(1, 1, -1))  # \"valid\" convolution\n",
    "    d_f = d_f[0, 0]  # discard dummy dims\n",
    "    d_sigma = (d_f * d_f_sigma).sum()\n",
    "    d_s = d_sigma * sigma_param_grad(s)\n",
    "    return d_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Experiment: Optimize $\\sigma$ to Recover Blur Kernel\n",
    "\n",
    "To illustrate the optimization of kernel size via sigma with a toy problem, let's recover the size of a Gaussian blur from smoothed data in 1D.\n",
    "\n",
    "1. Generate a random 1D signal and smooth it with a reference sigma.\n",
    "2. Instantiate our filter with zero initialization of the sigma parameter.\n",
    "3. Learn sigma by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100)\n",
    "true_sigma = torch.tensor([3.])\n",
    "true_blur, _ = gaussian_filter_with_grad(true_sigma)\n",
    "xf = F.conv1d(x.view(1, 1, -1), true_blur.view(1, 1, -1), padding=true_blur.size(0) // 2)[0, 0]\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('signal')\n",
    "plt.plot(x.numpy())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('smoothed')\n",
    "plt.plot(xf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recovery(xf, xf_hat, iter_):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.title(\"Recovery iter. {}\".format(iter_))\n",
    "    plt.plot(xf.numpy(), 'b', label='ref.')\n",
    "    plt.plot(xf_hat.numpy(), 'r', label='rec.')\n",
    "    plt.legend()\n",
    "    \n",
    "s = torch.tensor([0.])\n",
    "\n",
    "max_iter = 100\n",
    "step_size = torch.tensor([0.1])\n",
    "for iter_ in range(max_iter):\n",
    "    xf_hat, d_f_sigma = sigma_filter_forward(x, s)\n",
    "    # loss: squared error\n",
    "    loss = 0.5 * ((xf_hat - xf)**2).sum()\n",
    "    if iter_ % 10 == 0:\n",
    "        print('loss ', loss.item())\n",
    "    if iter_ in (0, 4, 16):\n",
    "        plot_recovery(xf, xf_hat, iter_)\n",
    "    d_loss = xf_hat - xf\n",
    "    d_s = sigma_filter_backward(x, s, d_loss, d_f_sigma)\n",
    "    # update\n",
    "    s -= step_size * d_s\n",
    "plot_recovery(xf, xf_hat, iter_ + 1)\n",
    "\n",
    "sigma_hat = sigma_from_param(s)\n",
    "print('\\ntrue sigma {:0.2f} recovered sigma {:0.2f}'.format(true_sigma.item(), sigma_hat.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the gradient by finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.tensor([1e-5])\n",
    "for _ in range(10):\n",
    "    s = torch.randn(1) * 3.\n",
    "    # forward-backward\n",
    "    xf_hat, d_f_sigma = sigma_filter_forward(x, s)\n",
    "    loss = 0.5 * ((xf_hat - xf)**2).sum()\n",
    "    d_loss = xf_hat - xf\n",
    "    d_s = sigma_filter_backward(x, s, d_loss, d_f_sigma)\n",
    "    \n",
    "    # forward +eps\n",
    "    xf_eps, _ = sigma_filter_forward(x, s + eps)\n",
    "    loss_ = 0.5 * ((xf_eps - xf)**2).sum()\n",
    "    d_s_check = (loss_ - loss) / eps\n",
    "    #print('loss ', loss.item(), loss_.item(), (loss - loss_).item())\n",
    "    #print('s ', s.item(), (s + eps).item(), (s - (s + eps)).item())\n",
    "    err = torch.abs(d_s_check - d_s)\n",
    "    print('analytic {: 09.5f} numerical {: 09.5f} error {:0.8f}'.format(d_s.item(), d_s_check.item(), err.item()))\n",
    "    assert(err < 10*eps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
